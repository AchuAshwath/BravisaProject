{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to compile index list and generate daily IRS report\n",
    "import datetime\n",
    "import requests\n",
    "import os.path\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import csv\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas.io.sql as sqlio\n",
    "import time\n",
    "import math\n",
    "from datetime import timedelta\n",
    "from utils.db_helper import DB_Helper\n",
    "import utils.date_set as date_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = DB_Helper().db_connect()\n",
    "cur = conn.cursor()\n",
    "dbURL = DB_Helper().engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sub_industry_index(conn, date):\n",
    "    \"\"\" Generate the Industry index\n",
    "\n",
    "    Operation:\n",
    "        Fetch the data from IndustryDivisor and IndustryList for current date,\n",
    "        and calculate the Open, close, high, low, PE , EPS and Earning Growth,\n",
    "\n",
    "        'open' = ff_open/divisor, 'high' = ff_high/divisor, 'low' = ff_low/divisor,\n",
    "\n",
    "        'close' = ff_close/divisor, 'earnings growth' = (eps – eps back / eps back) * 100 ,\n",
    "\n",
    "        'pe' = sum os close / ff earning, eps = ff earning / ff equity.\n",
    "\n",
    "    Return:\n",
    "        Data of Industry index.\n",
    "    \"\"\"\n",
    "      # Generating Industry index list\n",
    "\n",
    "    industry_divisor_sql = 'SELECT * FROM public.\"SubIndustryDivisor\" WHERE \"Date\" = \\''+date+'\\' ;'\n",
    "    industry_divisor = sqlio.read_sql_query(industry_divisor_sql, con = conn)\n",
    "\n",
    "    industry_ff_ohlc_sql = 'SELECT \"SubIndustryIndexName\", \"SubIndustry\", \\\n",
    "                        SUM(\"OS_Close\") AS sum_os_close,\\\n",
    "                        SUM(\"FF_Open\") AS ff_open_sum, \\\n",
    "                        SUM(\"FF_High\") AS ff_high_sum, \\\n",
    "                        SUM(\"FF_Low\") AS ff_low_sum, \\\n",
    "                        SUM(\"FF_Close\") AS ff_close_sum, \\\n",
    "                        SUM(\"Volume\") AS sum_vol, \\\n",
    "                        SUM(\"OS\") AS os_sum, \\\n",
    "                        SUM(\"PAT\") AS earnings, \\\n",
    "                        SUM(\"Equity\") AS sum_equity,   \\\n",
    "                        SUM(\"prev_pat\") AS prev_earnings, \\\n",
    "                        SUM(\"prev_equity\") AS prev_equity, \\\n",
    "                        count(*) AS company_count\\\n",
    "                        FROM public.\"IndustryList\" \\\n",
    "                        WHERE \"GenDate\" = \\''+date+'\\' AND \"SubIndustryIndexName\" is not null \\\n",
    "                        GROUP BY \"SubIndustryIndexName\", \"SubIndustry\" ;'\n",
    "    industry_ff_ohlc = sqlio.read_sql_query(industry_ff_ohlc_sql, con = conn)\n",
    "\n",
    "    for index, row in industry_ff_ohlc.iterrows():\n",
    "\n",
    "      ff_open_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['ff_open_sum']\n",
    "      ff_open = ff_open_list.item() if len(ff_open_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_high_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['ff_high_sum']\n",
    "      ff_high = ff_high_list.item() if len(ff_high_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_low_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['ff_low_sum']\n",
    "      ff_low = ff_low_list.item() if len(ff_low_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_close_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['ff_close_sum']\n",
    "      ff_close = ff_close_list.item() if len(ff_close_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_os_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['os_sum']\n",
    "      ff_os = ff_os_list.item() if len(ff_os_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_earnings_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['earnings']\n",
    "      ff_earnings = ff_earnings_list.item() if len(ff_earnings_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_equity_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['sum_equity']\n",
    "      ff_equity = ff_equity_list.item() if len(ff_equity_list.index) == 1 else np.nan\n",
    "\n",
    "      prev_earnings_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['prev_earnings']\n",
    "      prev_earnings = prev_earnings_list.item() if len(prev_earnings_list.index) == 1 else np.nan\n",
    "\n",
    "      prev_equity_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['prev_equity']\n",
    "      prev_equity = prev_equity_list.item() if len(prev_equity_list.index) == 1 else np.nan\n",
    "\n",
    "      divisor_list = industry_divisor.loc[(industry_divisor['IndexName']==row['SubIndustryIndexName'])]['Divisor']\n",
    "      divisor = divisor_list.item() if len(divisor_list.index) == 1 else np.nan\n",
    "\n",
    "      sum_os_close_list = industry_ff_ohlc.loc[(industry_ff_ohlc['SubIndustryIndexName']==row['SubIndustryIndexName'])]['sum_os_close']\n",
    "      sum_os_close = sum_os_close_list.item() if len(sum_os_close_list.index) == 1 else np.nan\n",
    "\n",
    "      if np.isnan(divisor) or divisor == 0:\n",
    "        divisor  =  10000\n",
    "        print(\"Devisor gen_Subindustry_index: \", divisor)\n",
    "\n",
    "      div_open = ff_open/divisor if divisor != 0 and not np.isnan(ff_open) else np.nan\n",
    "      div_high = ff_high/divisor if divisor != 0 and not np.isnan(ff_high) else np.nan\n",
    "      div_low = ff_low/divisor if divisor != 0 and not np.isnan(ff_low) else np.nan\n",
    "      div_close = ff_close/divisor if divisor != 0 and not np.isnan(ff_close) else np.nan\n",
    "\n",
    "\n",
    "      # div_open = ff_open/divisor if divisor != 0 and not np.isnan(ff_open) else np.nan\n",
    "      # div_open = ff_open/divisor if divisor and ff_open is not None and divisor != 0 else np.nan\n",
    "      # div_high = ff_high/divisor if divisor and ff_high is not None and divisor != 0 else np.nan\n",
    "      # div_low = ff_low/divisor if divisor and ff_low is not None and divisor != 0 else np.nan\n",
    "      # div_close = ff_close/divisor if divisor and ff_close is not None and divisor != 0 else np.nan\n",
    "\n",
    "\n",
    "      pe = (sum_os_close)/ff_earnings if ff_earnings !=0 else np.nan\n",
    "      eps = ff_earnings/ff_equity if ff_equity !=0 else np.nan\n",
    "      eps_back = prev_earnings/prev_equity if prev_equity !=0 else np.nan\n",
    "      earnings_growth = ((eps-eps_back)/abs(eps_back))*100 if eps_back != 0 else np.nan\n",
    "\n",
    "\n",
    "      industry_ff_ohlc.loc[index, 'Open'] = div_open\n",
    "      industry_ff_ohlc.loc[index, 'High'] = div_high\n",
    "      industry_ff_ohlc.loc[index, 'Low'] = div_low\n",
    "      industry_ff_ohlc.loc[index, 'Close'] = div_close\n",
    "      industry_ff_ohlc.loc[index, 'PE'] = pe\n",
    "      industry_ff_ohlc.loc[index, 'EPS'] = eps\n",
    "      industry_ff_ohlc.loc[index, 'Earnings Growth'] = earnings_growth\n",
    "\n",
    "\n",
    "    return industry_ff_ohlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_industry_index(conn, date):\n",
    "    \"\"\" Generate the Industry index\n",
    "\n",
    "    Operation:\n",
    "        Fetch the data from IndustryDivisor and IndustryList for current date,\n",
    "        and calculate the Open, close, high, low, PE , EPS and Earning Growth,\n",
    "\n",
    "        'open' = ff_open/divisor, 'high' = ff_high/divisor, 'low' = ff_low/divisor,\n",
    "\n",
    "        'close' = ff_close/divisor, 'earnings growth' = (eps – eps back / eps back) * 100 ,\n",
    "\n",
    "        'pe' = sum os close / ff earning, eps = ff earning / ff equity.\n",
    "\n",
    "    Return:\n",
    "        Data of Industry index.\n",
    "    \"\"\"\n",
    "      # Generating Industry index list\n",
    "\n",
    "    industry_divisor_sql = 'SELECT * FROM public.\"IndustryDivisor\" WHERE \"Date\" = \\''+date+'\\' ;'\n",
    "    industry_divisor = sqlio.read_sql_query(industry_divisor_sql, con = conn)\n",
    "\n",
    "    industry_ff_ohlc_sql = 'SELECT \"IndustryIndexName\", \"Industry\", \\\n",
    "                        SUM(\"OS_Close\") AS sum_os_close,\\\n",
    "                        SUM(\"FF_Open\") AS ff_open_sum, \\\n",
    "                        SUM(\"FF_High\") AS ff_high_sum, \\\n",
    "                        SUM(\"FF_Low\") AS ff_low_sum, \\\n",
    "                        SUM(\"FF_Close\") AS ff_close_sum, \\\n",
    "                        SUM(\"Volume\") AS sum_vol, \\\n",
    "                        SUM(\"OS\") AS os_sum, \\\n",
    "                        SUM(\"PAT\") AS earnings, \\\n",
    "                        SUM(\"Equity\") AS sum_equity,   \\\n",
    "                        SUM(\"prev_pat\") AS prev_earnings, \\\n",
    "                        SUM(\"prev_equity\") AS prev_equity, \\\n",
    "                        count(*) AS company_count\\\n",
    "                        FROM public.\"IndustryList\" \\\n",
    "                        WHERE \"GenDate\" = \\''+date+'\\' AND \"IndustryIndexName\" is not null \\\n",
    "                        GROUP BY \"IndustryIndexName\", \"Industry\" ;'\n",
    "    industry_ff_ohlc = sqlio.read_sql_query(industry_ff_ohlc_sql, con = conn)\n",
    "\n",
    "    for index, row in industry_ff_ohlc.iterrows():\n",
    "\n",
    "      ff_open_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['ff_open_sum']\n",
    "      ff_open = ff_open_list.item() if len(ff_open_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_high_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['ff_high_sum']\n",
    "      ff_high = ff_high_list.item() if len(ff_high_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_low_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['ff_low_sum']\n",
    "      ff_low = ff_low_list.item() if len(ff_low_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_close_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['ff_close_sum']\n",
    "      ff_close = ff_close_list.item() if len(ff_close_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_os_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['os_sum']\n",
    "      ff_os = ff_os_list.item() if len(ff_os_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_earnings_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['earnings']\n",
    "      ff_earnings = ff_earnings_list.item() if len(ff_earnings_list.index) == 1 else np.nan\n",
    "\n",
    "      ff_equity_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['sum_equity']\n",
    "      ff_equity = ff_equity_list.item() if len(ff_equity_list.index) == 1 else np.nan\n",
    "\n",
    "      prev_earnings_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['prev_earnings']\n",
    "      prev_earnings = prev_earnings_list.item() if len(prev_earnings_list.index) == 1 else np.nan\n",
    "\n",
    "      prev_equity_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['prev_equity']\n",
    "      prev_equity = prev_equity_list.item() if len(prev_equity_list.index) == 1 else np.nan\n",
    "\n",
    "      divisor_list = industry_divisor.loc[(industry_divisor['IndexName']==row['IndustryIndexName'])]['Divisor']\n",
    "      divisor = divisor_list.item() if len(divisor_list.index) == 1 else np.nan\n",
    "\n",
    "      sum_os_close_list = industry_ff_ohlc.loc[(industry_ff_ohlc['IndustryIndexName']==row['IndustryIndexName'])]['sum_os_close']\n",
    "      sum_os_close = sum_os_close_list.item() if len(sum_os_close_list.index) == 1 else np.nan\n",
    "\n",
    "      if np.isnan(divisor) or divisor == 0:\n",
    "        print(\"Devisor gen_industry_index: \", divisor)\n",
    "\n",
    "\n",
    "      div_open = ff_open/divisor if divisor != 0 and not np.isnan(ff_open) else np.nan\n",
    "      div_high = ff_high/divisor if divisor != 0 and not np.isnan(ff_high) else np.nan\n",
    "      div_low = ff_low/divisor if divisor != 0 and not np.isnan(ff_low) else np.nan\n",
    "      div_close = ff_close/divisor if divisor != 0 and not np.isnan(ff_close) else np.nan\n",
    "\n",
    "\n",
    "      pe = (sum_os_close)/ff_earnings if ff_earnings !=0 else np.nan\n",
    "      eps = ff_earnings/ff_equity if ff_equity !=0 else np.nan\n",
    "      eps_back = prev_earnings/prev_equity if prev_equity !=0 else np.nan\n",
    "      earnings_growth = ((eps-eps_back)/abs(eps_back))*100 if eps_back != 0 else np.nan\n",
    "\n",
    "\n",
    "      industry_ff_ohlc.loc[index, 'Open'] = div_open\n",
    "      industry_ff_ohlc.loc[index, 'High'] = div_high\n",
    "      industry_ff_ohlc.loc[index, 'Low'] = div_low\n",
    "      industry_ff_ohlc.loc[index, 'Close'] = div_close\n",
    "      industry_ff_ohlc.loc[index, 'PE'] = pe\n",
    "      industry_ff_ohlc.loc[index, 'EPS'] = eps\n",
    "      industry_ff_ohlc.loc[index, 'Earnings Growth'] = earnings_growth\n",
    "\n",
    "\n",
    "    return industry_ff_ohlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.strptime('2020-01-01', \"%Y-%m-%d\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ind = gen_industry_index(conn,'2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ind = gen_sub_industry_index(conn, '2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rank_index(industry_index, sub_industry_index,conn, date):\n",
    "    \"\"\" calculating the rank for sector, subsector and industry.\n",
    "\n",
    "    Args:\n",
    "        sector_index = data of Open, close, high, low, PE , EPS\n",
    "                      and Earning Growth of sector index.\n",
    "\n",
    "        subsector_index = data of Open, close, high, low, PE , EPS\n",
    "                        and Earning Growth of subsector index.\n",
    "\n",
    "        industry_index = data of Open, close, high, low, PE , EPS\n",
    "                        and Earning Growth of industry index.\n",
    "\n",
    "    Operation:\n",
    "        Fetch the data from IndexHistory and calculate the value for six month change\n",
    "        for sector index, subsector index and industry index,\n",
    "        change six month = (curr close - prev close / prev close)* 100\n",
    "        index = change * 100.\n",
    "\n",
    "    Return:\n",
    "        Rank value of Sector Index, SubSector Index and Industry index.\n",
    "    \"\"\"\n",
    "\n",
    "    six_month_back = '01-06-2019'\n",
    "\n",
    "    indexlist_history_sql = 'SELECT DISTINCT ON(\"TICKER\") * FROM public.\"IndexHistory\" \\\n",
    "                            where \"DATE\" <= \\''+six_month_back+'\\' \\\n",
    "                            ORDER BY \"TICKER\", \"DATE\" DESC; '\n",
    "    index_history = sqlio.read_sql_query(indexlist_history_sql, con = conn)\n",
    "\n",
    "\n",
    "    #Sector rank\n",
    "    # if not(sector_index.empty):\n",
    "\n",
    "    #   for index, row in sector_index.iterrows():\n",
    "\n",
    "    #     prev_close_list = index_history.loc[(index_history['TICKER']==row['SectorIndexName'])]['CLOSE']\n",
    "    #     prev_close = prev_close_list.item() if len(prev_close_list.index) == 1 else np.nan\n",
    "\n",
    "    #     current_close_list = sector_index.loc[(sector_index['SectorIndexName']==row['SectorIndexName'])]['Close']\n",
    "    #     current_close = current_close_list.item() if len(current_close_list.index) == 1 else np.nan\n",
    "\n",
    "    #     change_six_months = (current_close-prev_close)/prev_close * 100 if prev_close != np.nan else 0\n",
    "\n",
    "    #     sector_index.loc[index, 'Change'] = change_six_months\n",
    "\n",
    "    #   sector_index['Rank'] = sector_index['Change'].rank(ascending=True, pct=True) * 100\n",
    "\n",
    "    #   sector_index = sector_index.rename(columns = {\"SectorIndexName\":\"IndexName\", \"Sector\": \"Index\"})\n",
    "\n",
    "    # else:\n",
    "\n",
    "    #   print(\"No Sector data\")\n",
    "\n",
    "\n",
    "    # #SubSector Rank\n",
    "    # if not(subsector_index.empty):\n",
    "\n",
    "    #   for index, row in subsector_index.iterrows():\n",
    "\n",
    "    #     prev_close_list = index_history.loc[(index_history['TICKER']==row['SubSectorIndexName'])]['CLOSE']\n",
    "    #     prev_close = prev_close_list.item() if len(prev_close_list.index) == 1 else np.nan\n",
    "\n",
    "    #     current_close_list = subsector_index.loc[(subsector_index['SubSectorIndexName']==row['SubSectorIndexName'])]['Close']\n",
    "    #     current_close = current_close_list.item() if len(current_close_list.index) == 1 else np.nan\n",
    "\n",
    "    #     change_six_months = (current_close-prev_close)/prev_close * 100 if prev_close != np.nan else 0\n",
    "\n",
    "    #     subsector_index.loc[index, 'Change'] = change_six_months\n",
    "\n",
    "    #   subsector_index['Rank'] = subsector_index['Change'].rank(ascending=True, pct=True) * 100\n",
    "\n",
    "    #   subsector_index = subsector_index.rename(columns = {\"SubSectorIndexName\":\"IndexName\", \"SubSector\": \"Index\"})\n",
    "\n",
    "    # else:\n",
    "\n",
    "    #   print(\"No Subsector data\")\n",
    "\n",
    "\n",
    "    #Industry Rank\n",
    "    if not(industry_index.empty):\n",
    "\n",
    "      for index, row in industry_index.iterrows():\n",
    "\n",
    "        prev_close_list = index_history.loc[(index_history['TICKER']==row['IndustryIndexName'])]['CLOSE']\n",
    "        prev_close = prev_close_list.item() if len(prev_close_list.index) == 1 else np.nan\n",
    "\n",
    "        current_close_list = industry_index.loc[(industry_index['IndustryIndexName']==row['IndustryIndexName'])]['Close']\n",
    "        current_close = current_close_list.item() if len(current_close_list.index) == 1 else np.nan\n",
    "\n",
    "        change_six_months = (current_close-prev_close)/prev_close * 100 if prev_close != np.nan else 0\n",
    "\n",
    "        industry_index.loc[index, 'Change'] = change_six_months\n",
    "\n",
    "      industry_index['Rank'] = industry_index['Change'].rank(ascending=True, pct=True) * 100\n",
    "\n",
    "      industry_index = industry_index.rename(columns = {\"IndustryIndexName\":\"IndexName\", \"Industry\": \"Index\"})\n",
    "\n",
    "    else:\n",
    "\n",
    "      print(\"No Industry data\")\n",
    "      \n",
    "    #SubIndustry Rank\n",
    "    if not(sub_industry_index.empty):\n",
    "    \n",
    "      for index, row in sub_industry_index.iterrows():\n",
    "\n",
    "        prev_close_list = index_history.loc[(index_history['TICKER']==row['SubIndustryIndexName'])]['CLOSE']\n",
    "        prev_close = prev_close_list.item() if len(prev_close_list.index) == 1 else np.nan\n",
    "\n",
    "        current_close_list = sub_industry_index.loc[(sub_industry_index['SubIndustryIndexName']==row['SubIndustryIndexName'])]['Close']\n",
    "        current_close = current_close_list.item() if len(current_close_list.index) == 1 else np.nan\n",
    "\n",
    "        change_six_months = (current_close-prev_close)/prev_close * 100 if prev_close != np.nan else 0\n",
    "\n",
    "        sub_industry_index.loc[index, 'Change'] = change_six_months\n",
    "\n",
    "      sub_industry_index['Rank'] = sub_industry_index['Change'].rank(ascending=True, pct=True) * 100\n",
    "\n",
    "      sub_industry_index = sub_industry_index.rename(columns = {\"SubIndustryIndexName\":\"IndexName\", \"SubIndustry\": \"Index\"})\n",
    "\n",
    "    else:\n",
    "\n",
    "      print(\"No SubIndustry data\")\n",
    "\n",
    "\n",
    "    irs_list = pd.concat([industry_index, sub_industry_index], axis=0, sort=True)\n",
    "    print('irslist: ',irs_list)\n",
    "\n",
    "\n",
    "    return irs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = calc_rank_index(ind_ind,sub_ind, conn, '01-01-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils.db_helper import DB_Helper\n",
    "import psycopg2\n",
    "\n",
    "missing_dir = os.path.join(os.getcwd(), 'missing')\n",
    "\n",
    "listOfcsv = {}\n",
    "\n",
    "conn = DB_Helper().db_connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "# recursively iterate over missing\n",
    "for root, dirs, files in os.walk(missing_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\") and not root == 'c:\\\\BravisaProject\\\\app\\missing\\\\Bravisa FullData1997 to Sept 2016' and not file.startswith('.') and not root=='c:\\\\BravisaProject\\\\app\\missing\\\\FB2904202402':\n",
    "            listOfcsv[file] = file\n",
    "print(listOfcsv)\n",
    "for root, dirs, files in os.walk(missing_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\") and not root == 'c:\\\\BravisaProject\\\\app\\missing\\\\Bravisa FullData1997 to Sept 2016' and not file.startswith('.') and not root=='c:\\\\BravisaProject\\\\app\\missing\\\\FB2904202402' and not file.startswith('Missing'):\n",
    "            \n",
    "            if file.endswith(\".csv\") and file in listOfcsv:\n",
    "            #print file path\n",
    "                print(os.path.join(root, file))\n",
    "                \n",
    "                # if file == 'ConsolidatedHalfyearlyResults.csv':\n",
    "                #     print(file)                    \n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"ConsolidatedHalfYearlyResults\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_consolidatedhalfyearlyresults()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                \n",
    "                # if file == 'ConsolidatedNinemonthsResults.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"ConsolidatedNineMonthsResults\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_consolidatedninemonthsresults()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'HalfyearlyResults.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"HalfYearlyResults\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_halfyearlyresults()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                if file == 'NinemonthsResults.csv':\n",
    "                    print(file)\n",
    "                    #insert into database\n",
    "                    try:\n",
    "                        copy_sql = \"\"\"\n",
    "                        COPY \"public\".\"NinemonthsResults\" FROM stdin WITH CSV HEADER\n",
    "                        DELIMITER as ','\n",
    "                        \"\"\"\n",
    "                        with open(os.path.join(root,file), 'r') as f:\n",
    "                            cur.copy_expert(sql=copy_sql, file=f)\n",
    "                            conn.commit()\n",
    "                            f.close()    \n",
    "                    except:\n",
    "                        print(\"Error in insert_ninemonthsresults()\", flush = True)\n",
    "                        conn.rollback()\n",
    "                        import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'FinanceBankingVI.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"FinanceBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_financebankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                \n",
    "                # if file == 'FinanceConsolidatedBankingVI.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"FinanceConsolidatedBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_financeconsolidatedbankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'FinanceConsolidatedNonBankingVI.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"FinanceConsolidatedNonBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_financeconsolidatednonbankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'FinanceNonBankingVI.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"FinanceNonBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_financenonbankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                \n",
    "                # if file == 'RatiosBankingVI.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"RatiosBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_ratiosbankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'RatiosConsolidatedBankingVI.csv': \n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"RatiosConsolidatedBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_ratiosconsolidatedbankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'RatiosConsolidatedNonBankingVI.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"RatiosConsolidatedNonBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_ratiosconsolidatednonbankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'RatiosNonBankingVI.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"RatiosNonBankingVI\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_ratiosnonbankingvi()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        \n",
    "                # if file == 'SchemeNAVDetails.csv':\n",
    "                #     print(file)\n",
    "                #     #insert into database\n",
    "                #     try:\n",
    "                #         copy_sql = \"\"\"\n",
    "                #         COPY \"public\".\"SchemeNAVDetails\" FROM stdin WITH CSV HEADER\n",
    "                #         DELIMITER as ','\n",
    "                #         \"\"\"\n",
    "                #         with open(os.path.join(root,file), 'r') as f:\n",
    "                #             cur.copy_expert(sql=copy_sql, file=f)\n",
    "                #             conn.commit()\n",
    "                #             f.close()    \n",
    "                #     except:\n",
    "                #         print(\"Error in insert_schemenavdetails()\", flush = True)\n",
    "                #         conn.rollback()\n",
    "                #         import traceback; traceback.print_exc()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from utils.db_helper import DB_Helper\n",
    "import psycopg2\n",
    "\n",
    "missing_dir = os.path.join(os.getcwd(), 'missing')\n",
    "\n",
    "#insert missingresults\n",
    "conn = DB_Helper().db_connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "#insert MissingQuarterlyResults\n",
    "filename = missing_dir + '/MissingQuarterlyResults.csv' \n",
    "try:\n",
    "    copy_sql = \"\"\"\n",
    "    COPY \"public\".\"QuarterlyResults\" FROM stdin WITH CSV HEADER\n",
    "    DELIMITER as ','\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        cur.copy_expert(sql=copy_sql, file=f)\n",
    "        conn.commit()\n",
    "        f.close()\n",
    "except:\n",
    "    print(\"Error in insert_missingquarterlyresults()\", flush = True)\n",
    "    conn.rollback()\n",
    "    import traceback; traceback.print_exc()\n",
    "\n",
    "#insert MissingConsolidatedQuarterlyResults\n",
    "filename = missing_dir + '/MissingConsolidatedQuarterlyResults.csv' \n",
    "\n",
    "try:\n",
    "    copy_sql = \"\"\"\n",
    "    COPY \"public\".\"ConsolidatedQuarterlyResults\" FROM stdin WITH CSV HEADER\n",
    "    DELIMITER as ','\n",
    "    \"\"\"\n",
    "    with open(filename\n",
    "                , 'r') as f:\n",
    "            cur.copy_expert(sql=copy_sql, file=f)\n",
    "            conn.commit()\n",
    "            f.close()       \n",
    "except:\n",
    "    print(\"Error in insert_missingconsolidatedquarterlyresults()\", flush = True)\n",
    "    conn.rollback()\n",
    "    import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete scripts for reports\n",
    "def delete_EPS(conn, cur, start_date, end_date):\n",
    "\ttry:\n",
    "\n",
    "\t\teps_sql = \"\"\"DELETE FROM \"Reports\".\"EPS\"\n",
    "\t\t\t\t\tWHERE \"EPSDate\" <= %s AND \"EPSDate\" >= %s\"\"\" \n",
    "\t\tcur.execute(eps_sql, (end_date, start_date))\t\t\n",
    "\t\tconn.commit()\n",
    "\t\tprint(\"Deleted EPS data\")\t\n",
    "\t\tSTANDALONE_EPS_sql = \"\"\" DELETE FROM \"Reports\".\"STANDALONE_EPS\"\n",
    "\t\t\t\t\tWHERE \"EPSDate\" <= %s AND \"EPSDate\" >= %s\"\"\"\n",
    "\t\tcur.execute(STANDALONE_EPS_sql, (end_date, start_date))\n",
    "\t\tconn.commit()\t\n",
    "\t\tprint(\"Deleted STANDALONE_EPS data\")\n",
    "\t\tConsolidated_EPS_sql = \"\"\" DELETE FROM \"Reports\".\"Consolidated_EPS\"\n",
    "\t\t\t\t\tWHERE \"EPSDate\" <= %s AND \"EPSDate\" >= %s\"\"\"\n",
    "\t\tcur.execute(Consolidated_EPS_sql, (end_date, start_date))\n",
    "\t\tconn.commit()\n",
    "\t\tprint(\"Deleted Consolidated_EPS data\")\n",
    "\t\tQaurterlyEPS_sql = \"\"\" DELETE FROM public.\"QuarterlyEPS\"\n",
    "\t\t\t\t\tWHERE \"YearEnding\" >= %s AND \"YearEnding\" <= %s\"\"\"\n",
    "\t\tcur.execute(QaurterlyEPS_sql, (start_date, end_date))\n",
    "\t\tconn.commit()\n",
    "\t\tprint(\"Deleted QuarterlyEPS data\")\n",
    "\t\tConsolidatedQuarterlyEPS_sql = \"\"\" DELETE FROM public.\"ConsolidatedQuarterlyEPS\"\n",
    "\t\t\t\t\tWHERE \"YearEnding\" >= %s AND \"YearEnding\" <= %s\"\"\"\n",
    "\t\tcur.execute(ConsolidatedQuarterlyEPS_sql, (start_date,end_date))\n",
    "\t\tconn.commit()\n",
    "\t\tprint(\"Deleted ConsolidatedQuarterlyEPS data\")\n",
    "\t\tTTMsql = \"\"\" DELETE FROM public.\"TTM\"\n",
    "\t\t\t\t\tWHERE \"YearEnding\" >= %s AND \"YearEnding\" <= %s\"\"\"\n",
    "\t\tcur.execute(TTMsql, (start_date, end_date))\n",
    "\t\tconn.commit()\t\n",
    "\t\tprint(\"Deleted TTM data\")\n",
    "\t\tConsolidatedTTMsql = \"\"\" DELETE FROM public.\"ConsolidatedTTM\"\n",
    "\t\t\t\t\tWHERE \"YearEnding\" >= %s AND \"YearEnding\" <= %s\"\"\"\n",
    "\t\tcur.execute(ConsolidatedTTMsql, (start_date, end_date))\n",
    "\t\tconn.commit()\n",
    "\t\tprint(\"Deleted ConsolidatedTTM data\")\t\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error deleting EPS data: {e}\")\n",
    "\t\timport traceback; traceback.print_exc()\t\n",
    "  \n",
    "def delete_smr(conn,cur, start_date, end_date):\n",
    "    try:\n",
    "        SMRsql = \"\"\" DELETE FROM \"Reports\".\"SMR\"\n",
    "                    WHERE \"SMRDate\" >= %s AND \"SMRDate\" <= %s\"\"\"\n",
    "        cur.execute(SMRsql, (start_date, end_date))\n",
    "        conn.commit()\n",
    "        print(\"Deleted SMR data\")\n",
    "        RatiosMergeListsql = \"\"\" DELETE FROM public.\"RatiosMergeList\"\n",
    "                    WHERE \"GenDate\" >= %s AND \"GenDate\" <= %s\"\"\"\n",
    "        cur.execute(RatiosMergeListsql, (start_date, end_date))\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting SMR data: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        \n",
    "def delPRS(start_date, end_date, conn, cur):\n",
    "    try:\n",
    "        sql_PRS = \"\"\"DELETE FROM \"Reports\".\"PRS\" \n",
    "                    WHERE \"Date\" >= %s AND \"Date\" <= %s\"\"\"\n",
    "        cur.execute(sql_PRS, (start_date, end_date))\n",
    "        conn.commit()\n",
    "        print(\"Deleted the records from PRS table for the date range: \", start_date, \" to \", end_date)\n",
    "        \n",
    "        sql_PE = \"\"\"DELETE FROM \"PE\" \n",
    "                    WHERE \"GenDate\" >= %s AND \"GenDate\" <= %s\"\"\"\n",
    "        cur.execute(sql_PE, (start_date, end_date))\n",
    "        conn.commit()\n",
    "        print(\"Deleted the records from PE table for the date range: \", start_date, \" to \", end_date)\n",
    "        \n",
    "        sql_NHNL = \"\"\"DELETE FROM \"Reports\".\"NewHighNewLow\" \n",
    "                    WHERE \"Date\" >= %s AND \"Date\" <= %s\"\"\"\n",
    "        cur.execute(sql_NHNL, (start_date, end_date))\n",
    "        conn.commit()\n",
    "        print(\"Deleted the records from NHNL table for the date range: \", start_date, \" to \", end_date)\n",
    "    except Exception as e: \n",
    "        print(f\"Error deleting PRS data: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "from utils.db_helper import DB_Helper\n",
    "import psycopg2\n",
    "\n",
    "conn = DB_Helper().db_connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "start_date = '2024-12-20'\n",
    "end_date = '2024-12-27'\n",
    "\n",
    "delete_EPS(conn, cur, start_date, end_date)\n",
    "delete_smr(conn, cur, start_date, end_date)\n",
    "delPRS(start_date, end_date, conn, cur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xlsx = pd.read_excel('C:\\\\BravisaBackup\\\\MF Scheme code working.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "column_names = ['scheme_code', 'scheme_name', 'scheme_category', 'date', 'btt_scheme_code', 'btt_scheme_category']\n",
    "\n",
    "xlsx.columns = column_names\n",
    "\n",
    "xlsx.to_csv('C:\\\\BravisaBackup\\\\mf_category_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "#import timedelta\n",
    "from datetime import timedelta\n",
    "from lib import  index_ohlc, split_bonus\n",
    "from Test_reports import ohlc\n",
    "#ignore warnings\n",
    "import warnings\n",
    "from Test_reports.IRSchecker import IRS\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def delIRS(curr_date, conn, cur):\n",
    "    sql_IndustryList = \"\"\"delete from \"IndustryList\" \n",
    "                        where \"GenDate\" = %s\"\"\"\n",
    "    # execute the query with curr_date\n",
    "    cur.execute(sql_IndustryList, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from IndustryList table for the date: \", curr_date)\n",
    "    sql_SectorDivisor = \"\"\"delete from \"SectorDivisor\"\n",
    "                            where \"Date\" = %s\"\"\"\n",
    "    cur.execute(sql_SectorDivisor, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from SectorDivisor table for the date: \", curr_date)\n",
    "    sql_SubSecotDivisor = \"\"\"delete from \"SubSectorDivisor\" \n",
    "                            where \"Date\" = %s\"\"\"\n",
    "    cur.execute(sql_SubSecotDivisor, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from SubSectorDivisor table for the date: \", curr_date)\n",
    "    sql_IndustryDivisor = \"\"\"delete from \"IndustryDivisor\" \n",
    "                            where \"Date\" = %s\"\"\"\n",
    "    cur.execute(sql_IndustryDivisor, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from IndustryDivisor table for the date: \", curr_date)\n",
    "    sql_SubIndustryDivisor = \"\"\"delete from \"SubIndustryDivisor\" \n",
    "                                where \"Date\" = %s\"\"\"\n",
    "    cur.execute(sql_SubIndustryDivisor, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from SubIndustryDivisor table for the date: \", curr_date)\n",
    "    sql_SectorIndexList = \"\"\"delete from \"SectorIndexList\" \n",
    "                            where \"GenDate\" = %s\"\"\"\n",
    "    cur.execute(sql_SectorIndexList, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from SectorIndexList table for the date: \", curr_date)\n",
    "    sql_SubSectorIndexList = \"\"\"delete from \"SubSectorIndexList\" \n",
    "                                where \"GenDate\" = %s\"\"\"\n",
    "    cur.execute(sql_SubSectorIndexList, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from SubSectorIndexList table for the date: \", curr_date)\n",
    "    sql_IndustryIndexList = \"\"\"delete from \"IndustryIndexList\" \n",
    "                                where \"GenDate\" = %s\"\"\"\n",
    "    cur.execute(sql_IndustryIndexList, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from IndustryIndexList table for the date: \", curr_date)\n",
    "    sql_SubIndustryIndexList = \"\"\"delete from \"SubIndustryIndexList\" \n",
    "                                    where \"GenDate\" = %s\"\"\" \n",
    "    cur.execute(sql_SubIndustryIndexList, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from SubIndustryIndexList table for the date: \", curr_date)\n",
    "    sql_IRS = \"\"\"delete from \"Reports\".\"IRS\" \n",
    "                where \"GenDate\" = %s\"\"\"\n",
    "    cur.execute(sql_IRS, (curr_date,))\n",
    "    conn.commit()\n",
    "    print(\"Deleted the records from IRS table for the date: \", curr_date)\n",
    "    sql_IndexHistory = \"\"\"delete from \"IndexHistory\"  \n",
    "                        where \"DATE\" = %s \"\"\"\n",
    "    cur.execute(sql_IndexHistory, (curr_date,))\n",
    "    conn.commit() \n",
    "    \n",
    "def insert_shareholding(conn, cur,fbname ):\n",
    "    \"\"\" Insert the ShareHolding data into database\n",
    "\n",
    "    Operation:\n",
    "        Set the path for csv file, and fetch the data from ShareHolding.csv file.\n",
    "        delete the data based on key column CompanyCode and SHPDate,  \n",
    "        Export data into 'ShareHoldingExport.csv' file and insert into ShareHolding Table.\n",
    "    \"\"\"\n",
    "    fb_name = fbname\n",
    "    file_path = os.path.join(os.getcwd(), \"FBFiles\")\n",
    "\n",
    "    fb_csv_path = os.path.join(file_path, fb_name) \n",
    "    fb_csv_file = os.path.join(fb_csv_path ,'ShareHolding.csv')\n",
    "    \n",
    "    file_to_check = file_path +'\\\\'+ fb_name + '\\\\' + 'ShareHolding.csv'\n",
    "    \n",
    "    # print(file_to_check)\n",
    "    \n",
    "    if(fbname == 'intermediate_insert'):\n",
    "        \n",
    "        file_to_check = 'C:\\\\Users\\\\dsram\\\\BravisaLocalDeploy\\\\BravisaFiles\\\\MissingData'+ '\\\\' + 'ShareHolding.csv'\n",
    "        fb_csv_file = file_to_check\n",
    "\n",
    "    if os.path.isfile(file_to_check):\n",
    "        try:\n",
    "            table = pd.read_csv(fb_csv_file, engine='python')\n",
    "\n",
    "            table['SHPDate'] = pd.to_datetime(table['SHPDate'], errors = 'ignore')\n",
    "            table['SHPDate'] = table['SHPDate'].apply(lambda dt: dt.strftime('%Y-%m-%d')if not pd.isnull(dt) else '')\n",
    "\n",
    "            table['ModifiedDate'] = pd.to_datetime(table['ModifiedDate'], errors = 'ignore')\n",
    "            table['ModifiedDate'] = table['ModifiedDate'].apply(lambda dt: dt.strftime('%Y-%m-%d')if not pd.isnull(dt) else '')\n",
    "            \n",
    "            #Update Logic - Deletes based on key columns \n",
    "            print(\"Executing delete logic\", flush = True)\n",
    "            table_to_delete = table.groupby(['CompanyCode', 'SHPDate'], as_index=False).count()\n",
    "            '''self.logfile.write(\"\\tIn function: \" + str(fb_csv_file.split(\"\\\\\")[-1].split(\".\")[0]) + \n",
    "                        \"\\t\\tTotal Loops:\" + str(int(table_to_delete.shape[0])) + \"\\t\\tFor file: \" + str(fbname) + \"\\n\")\n",
    "            self.logfile.flush()'''\n",
    "            for index,row in table_to_delete.iterrows():\n",
    "                # self.logfile.write(\"\\tIn function: \" + str(fb_csv_file.split(\"\\\\\")[-1].split(\".\")[0]) + \n",
    "                # \t\t\"\\t\\tTotal Loops:\" + str(int(table_to_delete.shape[0])) + \"\\t\\tFor file: \" + str(fbname) + \"\\n\")\n",
    "                # self.logfile.flush()\n",
    "                sql = 'DELETE FROM public.\"ShareHolding\" WHERE \"CompanyCode\" =\\'' +str(row['CompanyCode']) + '\\' AND  \"SHPDate\"=\\'' + str(row['SHPDate']) + '\\';'\n",
    "                ##print(sql, flush = True)\n",
    "                cur.execute(sql)\n",
    "                conn.commit()\n",
    "            \n",
    "\n",
    "            print(\"Inserting Data into ShareHolding\", flush = True)\n",
    "            exportfilename = \"ShareHoldingExport.csv\"\n",
    "            exportfile = open(exportfilename,\"w\",encoding='utf-8')\n",
    "            table.to_csv(exportfile, header=True, index=False, lineterminator='\\r')\n",
    "            exportfile.close()\n",
    "            \n",
    "            copy_sql = \"\"\"\n",
    "                COPY \"public\".\"ShareHolding\" FROM stdin WITH CSV HEADER\n",
    "                DELIMITER as ','\n",
    "                \"\"\"\n",
    "            with open(exportfilename, 'r') as f:\n",
    "                cur.copy_expert(sql=copy_sql, file=f)\n",
    "                conn.commit()\n",
    "                f.close()\n",
    "            os.remove(exportfilename)\n",
    "        except:\n",
    "            print(\"Error in insert_shareholding()\", flush = True)\n",
    "            conn.rollback()\n",
    "\n",
    "    else:\n",
    "        print(\"File not found: \" + fb_csv_file, flush = True)\n",
    "\n",
    "def delete_OHLC(conn, cur, curr_date, end_date):\n",
    "    try:\n",
    "        ohlc_sql = f\"\"\"DELETE FROM public.\"OHLC\" WHERE \"Date\" >= '{curr_date}' AND \"Date\" <= '{end_date}'\"\"\"\n",
    "        cur.execute(ohlc_sql)\n",
    "        conn.commit()\n",
    "        print(\"Deleted OHLC data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting OHLC data: {e}\")\n",
    "\n",
    "    try:\n",
    "        nse_sql = f\"\"\"DELETE FROM public.\"NSE\" WHERE \"TIMESTAMP\" >= '{curr_date}' AND \"TIMESTAMP\" <= '{end_date}'\"\"\"\n",
    "        cur.execute(nse_sql)\n",
    "        conn.commit()\n",
    "        print(\"Deleted NSE data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting NSE data: {e}\")\n",
    "\n",
    "    try:\n",
    "        bse_sql = f\"\"\"DELETE FROM public.\"BSE\" WHERE \"TRADING_DATE\" >= '{curr_date}' AND \"TRADING_DATE\" <= '{end_date}'\"\"\"\n",
    "        cur.execute(bse_sql)\n",
    "        conn.commit()\n",
    "        print(\"Deleted BSE data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting BSE data: {e}\")\n",
    "\n",
    "# Establish database connection\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"BravisaDB\",\n",
    "        user=\"postgres\",\n",
    "        password=\"edsols\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    date_range = pd.date_range(start='2020-08-01',end='2025-01-03')\n",
    "    start_date = date_range[0]\n",
    "    end_date = date_range[-1]\n",
    "    delete_OHLC(conn, cur, start_date, end_date)\n",
    "    date_format = \"%d%m%Y\"\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        if current_date.weekday() == 0:  # Monday\n",
    "            prev_date = current_date - timedelta(days=2)  # Saturday\n",
    "        else:\n",
    "            prev_date = current_date - timedelta(days=1)  # Previous day\n",
    "\n",
    "        print(current_date)\n",
    "        insert_shareholding(conn, cur, \"FB\" + prev_date.strftime(date_format) + \"03\")\n",
    "        insert_shareholding(conn, cur, \"FB\" + current_date.strftime(date_format) + \"01\")\n",
    "        insert_shareholding(conn, cur, \"FB\" + current_date.strftime(date_format) + \"02\")\n",
    "        print(\"Inserted: shareholding for : \", current_date)\n",
    "        try:\n",
    "            ohlc.main(current_date)\n",
    "            # ohlc_nse, ohlc_bse, ohlc_joined, ohlc_bg,  ohlc_full = ohlc.main(current_date)\n",
    "            split_bonus.main(current_date)\n",
    "            delIRS(current_date, conn, cur)    \n",
    "            btt_list, ohlc_irs, btt_ohlc_merge, ohlc_change, ohlc_backgroundinfo_merge, industry_merge, industry_list_calc_free_float, \\\n",
    "            industry_list_ff_ohlc, master_list, sector_divisor, subsector_divisor, industry_divisor, sub_industry_divisor, sector_divisor_index,\\\n",
    "            subsector_divisor_index, industry_divisor_index, sub_industry_divisor_index, sector_index, subsector_index, industry_index,\\\n",
    "            sub_industry_index,irs_list_rank, irs_list, irs_pe, industry_list = IRS().gen_irs_current(current_date,conn, cur)\n",
    "            \n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\" there is no file for the date: \", current_date)    \n",
    "        current_date += timedelta(days=1)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to the database: {e}\")\n",
    "    import traceback; traceback.print_exc();\n",
    "finally:\n",
    "    if cur:\n",
    "        cur.close()\n",
    "    if conn:\n",
    "        conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT distinct \"SecurityCode\" FROM public.\"SchemeNAVCurrentPrices\"\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from utils.db_helper import DB_Helper\n",
    "\n",
    "conn = DB_Helper().db_connect()\n",
    "cur = conn.cursor()\n",
    "min_len = 100\n",
    "\n",
    "sql = \"\"\"SELECT distinct \"SecurityCode\" FROM public.\"SchemeNAVCurrentPrices\" \"\"\"\n",
    "df = pd.read_sql_query(sql, conn)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df['SecurityCode'] = df['SecurityCode'].astype(str)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        print(row['SecurityCode'])\n",
    "        print(len(row['SecurityCode']))\n",
    "        print(min_len)\n",
    "        if len(row['SecurityCode']) < min_len:\n",
    "            min_len = len(row['SecurityCode'])\n",
    "\n",
    "print(min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting MFMergeList data for: 2025-01-02\n",
      "Deleting FRS MF Rank data for: 2025-01-02\n",
      "Deleting FRS-NAVRank data for: 2025-01-02\n",
      "Deleting FRS-NAV Category Avg data for: 2025-01-02\n",
      "deleting MF ohlc for: 2025-01-02\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "# from utils.report_delete import deleteFRS\n",
    "from utils.db_helper import DB_Helper    \n",
    "\n",
    "conn= DB_Helper().db_connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "start_date = '2025-01-02'\n",
    "end_date = '2025-01-02'\n",
    "\n",
    "def deleteFRS(curr_date, end_date):\n",
    "\n",
    "    conn = DB_Helper().db_connect()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    print(\"Deleting MFMergeList data for:\",curr_date)\n",
    "    mfmerge_sql = 'DELETE FROM public.\"MFMergeList\" WHERE \"GenDate\">=\\'' + (curr_date) + '\\' AND \"GenDate\" <=\\'' + (end_date) + '\\';'\n",
    "    cur.execute(mfmerge_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Deleting FRS MF Rank data for:\",curr_date)\n",
    "    frs_navrank_sql = 'DELETE FROM \"Reports\".\"FRS-MFRank\" WHERE \"Date\">=\\'' + (curr_date) + '\\' AND \"Date\" <= \\'' + (end_date) + '\\';'\n",
    "    cur.execute(frs_navrank_sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"Deleting FRS-NAVRank data for:\",curr_date)\n",
    "    frs_nav_sql = 'DELETE FROM \"Reports\".\"FRS-NAVRank\" WHERE \"Date\">=\\'' + (curr_date) + '\\' AND \"Date\" <= \\'' + (end_date) + '\\';'\n",
    "    cur.execute(frs_nav_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Deleting FRS-NAV Category Avg data for:\",curr_date)\n",
    "    frs_sql = 'DELETE FROM \"Reports\".\"FRS-NAVCategoryAvg\" WHERE \"Date\">=\\'' + (curr_date) + '\\' AND \"Date\" <= \\'' + (end_date) + '\\';'         \n",
    "    cur.execute(frs_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"deleting MF ohlc for:\", curr_date)\n",
    "    mfohlc_sql = 'delete from public.mf_ohlc where date >= \\'' + (curr_date) + '\\' AND date <= \\'' + (end_date) + '\\';'         \n",
    "    cur.execute(mfohlc_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "deleteFRS(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating MF List for today: 2025-01-02 00:00:00\n",
      "Filtering schemes from Scheme Master\n",
      "Length of scheme master list: 1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BravisaProject\\app\\reports\\FRS.py:57: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  scheme_master_list = sqlio.read_sql_query(scheme_master_sql, con = conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:67: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  scheme_aum_list = sqlio.read_sql_query(scheme_aum_sql, con = conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging filtered list with schemewise portfolio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BravisaProject\\app\\reports\\FRS.py:109: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  scheme_wise_portfolio_list = sqlio.read_sql_query(scheme_wise_portfolio_sql, con = conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging with Indutry mapping\n",
      "Calculating Market Cap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BravisaProject\\app\\reports\\FRS.py:132: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  industry_info = pd.read_sql_query(industry_info_sql, conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:160: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pe_list = sqlio.read_sql_query(pe_sql, con = conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:172: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Large Cap' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  scheme_mf_list.loc[index, 'Market Cap'] = market_cap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into MFList\n",
      "Compiled MF merge list\n",
      "Calculating MF Exposure and Rank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BravisaProject\\app\\reports\\FRS.py:245: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  mf_rank_unique_list = sqlio.read_sql_query(mf_rank_unique_sql, con = conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:257: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  stock_qty_list = sqlio.read_sql_query(stock_qty_sql, con = conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:265: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  shareholding_list = sqlio.read_sql_query(shareholding_sql, con = conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Size for MF:  1101\n",
      "Inserting into table\n",
      "Completed MF Rank generation\n",
      "\n",
      "Generating NAV Rank and category average for today: 2025-01-02 00:00:00\n",
      "Filtering schemes from scheme master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BravisaProject\\app\\reports\\FRS.py:345: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  scheme_list = sqlio.read_sql_query(scheme_sql, con = conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:353: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  scheme_aum_list = sqlio.read_sql_query(scheme_aum_sql, con = conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging with SchemeNavMaster\n",
      "Getting BTT MF Categories\n",
      "Getting Scheme NAV current prices data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BravisaProject\\app\\reports\\FRS.py:383: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  scheme_nav_master_list = sqlio.read_sql_query(scheme_nav_master_sql, con = conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:414: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  btt_mf_category = sqlio.read_sql_query(sql, con = conn)\n",
      "c:\\BravisaProject\\app\\reports\\FRS.py:448: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  scheme_nav_prices_list = sqlio.read_sql_query(scheme_nav_sql, con = conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.405532e+07\n",
      "1       1.405532e+07\n",
      "2       1.405129e+08\n",
      "3       1.405179e+07\n",
      "4       1.405179e+07\n",
      "            ...     \n",
      "1271    1.405183e+07\n",
      "1272    1.405157e+08\n",
      "1273    1.405162e+08\n",
      "1274    1.405168e+08\n",
      "1275    1.405167e+08\n",
      "Name: SecurityCode, Length: 1276, dtype: float64\n",
      "Calculating Scheme Rank\n",
      "Inserting Scheme NAV Rank in table\n",
      "Inserting Scheme NAV List: \n",
      "Calculating average of each group\n",
      "Inserting NAV Category Average in table\n",
      "Inserting Scheme NAV category average List: \n",
      "Completed NAV Rank and category average\n"
     ]
    }
   ],
   "source": [
    "from utils.db_helper import DB_Helper   \n",
    "from reports.FRS import FRS\n",
    "import pandas as pd\n",
    "\n",
    "conn = DB_Helper().db_connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "start_date = '2025-01-02'\n",
    "end_date = '2025-01-02'\n",
    "\n",
    "for curr_date in pd.date_range(start=start_date, end=end_date):\n",
    "        FRS().generate_current_mfrank(curr_date,conn,cur)\n",
    "        # scheme_master_nav, merge_scheme_nav, mf_category_mapping, scheme_nav_current_prices, scheme_rank, scheme_nav_category_avg = FRS().calc_nav_rank(conn,cur, curr_date)\n",
    "        FRS().generate_current_nav_rank(curr_date,conn,cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SecurityCode\n",
      "0  1.405145e+07\n",
      "1  1.234568e+07\n",
      "2  9.876543e+07\n",
      "SecurityCode    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal, ROUND_DOWN\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'SecurityCode': [14051447.002066001, 12345678.987654321, 98765432.123456789]}\n",
    "scheme_nav_merge_list = pd.DataFrame(data)\n",
    "\n",
    "# Truncate values to 6 decimal places without rounding\n",
    "scheme_nav_merge_list['SecurityCode'] = scheme_nav_merge_list['SecurityCode'].apply(\n",
    "    lambda x: Decimal(str(x)).quantize(Decimal('1.000000'), rounding=ROUND_DOWN)\n",
    ")\n",
    "scheme_nav_merge_list['SecurityCode'] = scheme_nav_merge_list['SecurityCode'].astype(np.float64)\n",
    "\n",
    "print(scheme_nav_merge_list)\n",
    "print(scheme_nav_merge_list.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting missing results till 2018 december\n",
    "from utils.db_helper import DB_Helper\n",
    "import pandas as pd\n",
    "\n",
    "db = DB_Helper()\n",
    "conn = db.db_connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "missing_dir = os.path.join(os.getcwd(), 'missing')\n",
    "\n",
    "date = datetime.strptime('2018-12-01', '%Y-%m-%d')\n",
    "\n",
    "MissingQuarterlyResults = pd.read_csv(os.path.join(missing_dir, 'MissingQuarterlyResults.csv'))\n",
    "MissingQuarterlyResults['ModifiedDate'] = pd.to_datetime(MissingQuarterlyResults['ModifiedDate']) \n",
    "# subset of MissingQuarterlyResulst but ModifiedDate less than 2018-12-01\n",
    "MissingQuarterlyResults = MissingQuarterlyResults[MissingQuarterlyResults['ModifiedDate'].dt.date <= date.date()]\n",
    "MissingConsolidatedQuarterlyResults = pd.read_csv(os.path.join(missing_dir, 'MissingConsolidatedQuarterlyResults.csv'))\n",
    "MissingConsolidatedQuarterlyResults['ModifiedDate'] = pd.to_datetime(MissingConsolidatedQuarterlyResults['ModifiedDate'])\n",
    "MissingConsolidatedQuarterlyResults = MissingConsolidatedQuarterlyResults[MissingConsolidatedQuarterlyResults['ModifiedDate'].dt.date <= date.date()]\n",
    "\n",
    "print(MissingQuarterlyResults.shape)\n",
    "print(MissingConsolidatedQuarterlyResults.shape)\n",
    "\n",
    "# fetch QuarterlyResults till 2018 december\n",
    "fetch_quarterlyresults_sql = \"\"\"SELECT * FROM public.\"QuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "fetch_consolidatedquarterlyresults_sql = \"\"\"SELECT * FROM public.\"ConsolidatedQuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "\n",
    "quarterlyresults = pd.read_sql_query(fetch_quarterlyresults_sql, conn)\n",
    "consolidatedquarterlyresults = pd.read_sql_query(fetch_consolidatedquarterlyresults_sql, conn)\n",
    "print(\"from db\")    \n",
    "print(quarterlyresults.shape)\n",
    "print(consolidatedquarterlyresults.shape)\n",
    "\n",
    "# concatenate the missing results with the fetched results\n",
    "quarterlyresults = pd.concat([quarterlyresults, MissingQuarterlyResults])\n",
    "consolidatedquarterlyresults = pd.concat([consolidatedquarterlyresults, MissingConsolidatedQuarterlyResults])\n",
    "print(\"after concatenation\")\n",
    "print(quarterlyresults.shape)\n",
    "print(consolidatedquarterlyresults.shape)\n",
    "\n",
    "#sort by modified date\n",
    "quarterlyresults = quarterlyresults.sort_values(by='ModifiedDate')\n",
    "consolidatedquarterlyresults = consolidatedquarterlyresults.sort_values(by='ModifiedDate')\n",
    "#drop duplicates\n",
    "quarterlyresults = quarterlyresults.drop_duplicates(['CompanyCode', 'YearEnding'], keep='last')\n",
    "consolidatedquarterlyresults = consolidatedquarterlyresults.drop_duplicates(['CompanyCode', 'YearEnding'], keep='last')\n",
    "print(\"after dropping duplicates\")\n",
    "print(quarterlyresults.shape)\n",
    "print(consolidatedquarterlyresults.shape)\n",
    "\n",
    "# delete the data from the tables\n",
    "delete_quarterlyresults_sql = \"\"\"DELETE FROM public.\"QuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "delete_consolidatedquarterlyresults_sql = \"\"\"DELETE FROM public.\"ConsolidatedQuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "try:\n",
    "    cur.execute(delete_quarterlyresults_sql)\n",
    "    cur.execute(delete_consolidatedquarterlyresults_sql)\n",
    "    conn.commit()\n",
    "    print(\"Deleted data till 2018-12-01\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting data: {e}\")\n",
    "    import traceback; traceback.print_exc()\n",
    "    conn.rollback()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting missing results till 2018 december\n",
    "from utils.db_helper import DB_Helper\n",
    "import pandas as pd\n",
    "\n",
    "db = DB_Helper()\n",
    "conn = db.db_connect()\n",
    "cur = conn.cursor()\n",
    "\n",
    "missing_dir = os.path.join(os.getcwd(), 'missing')\n",
    "\n",
    "date = datetime.strptime('2018-12-01', '%Y-%m-%d')\n",
    "\n",
    "MissingQuarterlyResults = pd.read_csv(os.path.join(missing_dir, 'MissingQuarterlyResults.csv'))\n",
    "MissingQuarterlyResults['ModifiedDate'] = pd.to_datetime(MissingQuarterlyResults['ModifiedDate']) \n",
    "# subset of MissingQuarterlyResulst but ModifiedDate less than 2018-12-01\n",
    "MissingQuarterlyResults = MissingQuarterlyResults[MissingQuarterlyResults['ModifiedDate'].dt.date <= date.date()]\n",
    "MissingConsolidatedQuarterlyResults = pd.read_csv(os.path.join(missing_dir, 'MissingConsolidatedQuarterlyResults.csv'))\n",
    "MissingConsolidatedQuarterlyResults['ModifiedDate'] = pd.to_datetime(MissingConsolidatedQuarterlyResults['ModifiedDate'])\n",
    "MissingConsolidatedQuarterlyResults = MissingConsolidatedQuarterlyResults[MissingConsolidatedQuarterlyResults['ModifiedDate'].dt.date <= date.date()]\n",
    "\n",
    "print(MissingQuarterlyResults.shape)\n",
    "print(MissingConsolidatedQuarterlyResults.shape)\n",
    "\n",
    "# fetch QuarterlyResults till 2018 december\n",
    "fetch_quarterlyresults_sql = \"\"\"SELECT * FROM public.\"QuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "fetch_consolidatedquarterlyresults_sql = \"\"\"SELECT * FROM public.\"ConsolidatedQuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "\n",
    "quarterlyresults = pd.read_sql_query(fetch_quarterlyresults_sql, conn)\n",
    "consolidatedquarterlyresults = pd.read_sql_query(fetch_consolidatedquarterlyresults_sql, conn)\n",
    "print(\"from db\")    \n",
    "print(quarterlyresults.shape)\n",
    "print(consolidatedquarterlyresults.shape)\n",
    "\n",
    "# concatenate the missing results with the fetched results\n",
    "quarterlyresults = pd.concat([quarterlyresults, MissingQuarterlyResults])\n",
    "consolidatedquarterlyresults = pd.concat([consolidatedquarterlyresults, MissingConsolidatedQuarterlyResults])\n",
    "print(\"after concatenation\")\n",
    "print(quarterlyresults.shape)\n",
    "print(consolidatedquarterlyresults.shape)\n",
    "\n",
    "#sort by modified date\n",
    "quarterlyresults = quarterlyresults.sort_values(by='ModifiedDate')\n",
    "consolidatedquarterlyresults = consolidatedquarterlyresults.sort_values(by='ModifiedDate')\n",
    "#drop duplicates\n",
    "quarterlyresults = quarterlyresults.drop_duplicates(['CompanyCode', 'YearEnding'], keep='last')\n",
    "consolidatedquarterlyresults = consolidatedquarterlyresults.drop_duplicates(['CompanyCode', 'YearEnding'], keep='last')\n",
    "print(\"after dropping duplicates\")\n",
    "print(quarterlyresults.shape)\n",
    "print(consolidatedquarterlyresults.shape)\n",
    "\n",
    "# delete the data from the tables\n",
    "delete_quarterlyresults_sql = \"\"\"DELETE FROM public.\"QuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "delete_consolidatedquarterlyresults_sql = \"\"\"DELETE FROM public.\"ConsolidatedQuarterlyResults\" WHERE \"ModifiedDate\" <= '2018-12-01'\"\"\"\n",
    "try:\n",
    "    cur.execute(delete_quarterlyresults_sql)\n",
    "    cur.execute(delete_consolidatedquarterlyresults_sql)\n",
    "    conn.commit()\n",
    "    print(\"Deleted data till 2018-12-01\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting data: {e}\")\n",
    "    import traceback; traceback.print_exc()\n",
    "    conn.rollback()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
